#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LLM å®¢æˆ·ç«¯å°è£…
æ”¯æŒ OpenAI å’Œå…¶ä»–å…¼å®¹ API
"""

import os
import json
import hashlib
from typing import Dict, Optional
from abc import ABC, abstractmethod


class BaseLLMClient(ABC):
    """LLMå®¢æˆ·ç«¯åŸºç±»"""
    
    @abstractmethod
    def chat(self, messages: list, temperature: float = 0.1, 
             response_format: str = "json") -> str:
        """å‘é€èŠå¤©è¯·æ±‚"""
        pass
    
    @abstractmethod
    def get_model_name(self) -> str:
        """è·å–æ¨¡å‹åç§°"""
        pass


class OpenAIClient(BaseLLMClient):
    """OpenAI API å®¢æˆ·ç«¯ï¼ˆChat Completions APIï¼‰"""
    
    def __init__(self, api_key: str = None, model: str = "gpt-4", 
                 base_url: str = None):
        """
        åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        
        Args:
            api_key: APIå¯†é’¥ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è·å–
            model: æ¨¡å‹åç§°
            base_url: APIåŸºç¡€URLï¼Œç”¨äºå…¼å®¹å…¶ä»–API
        """
        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError("è¯·å®‰è£… openai: pip install openai")
        
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("æœªé…ç½® OPENAI_API_KEY")
        
        self.model = model
        self.base_url = base_url
        
        if base_url:
            self.client = OpenAI(api_key=self.api_key, base_url=base_url)
        else:
            self.client = OpenAI(api_key=self.api_key)
    
    def chat(self, messages: list, temperature: float = 0.1,
             response_format: str = "json") -> str:
        """
        å‘é€èŠå¤©è¯·æ±‚
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            response_format: å“åº”æ ¼å¼ ("json" æˆ– "text")
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯ codex æ¨¡å‹ï¼Œéœ€è¦ä½¿ç”¨ Responses API
        if "codex" in self.model.lower():
            return self._chat_responses_api(messages, temperature, response_format)
        
        kwargs = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "timeout": 120,  # æ·»åŠ è¶…æ—¶è®¾ç½®
        }
        
        # GPT æ¨¡å‹æ”¯æŒ json_object æ ¼å¼
        if response_format == "json" and "gpt" in self.model.lower():
            kwargs["response_format"] = {"type": "json_object"}
        
        try:
            response = self.client.chat.completions.create(**kwargs)
            return response.choices[0].message.content
        except Exception as e:
            # å¦‚æœ response_format ä¸æ”¯æŒï¼Œé‡è¯•ä¸å¸¦è¯¥å‚æ•°
            if "response_format" in kwargs and "response_format" in str(e).lower():
                del kwargs["response_format"]
                response = self.client.chat.completions.create(**kwargs)
                return response.choices[0].message.content
            raise
    
    def _chat_responses_api(self, messages: list, temperature: float = 0.1,
                            response_format: str = "json") -> str:
        """
        ä½¿ç”¨ Responses API å‘é€è¯·æ±‚ï¼ˆç”¨äº codex æ¨¡å‹ï¼‰
        
        Responses API ç«¯ç‚¹: POST /v1/responses
        """
        import requests
        
        # æ„å»ºè¯·æ±‚
        url = self.base_url.rstrip('/') + '/responses' if self.base_url else 'https://api.openai.com/v1/responses'
        
        # ä» messages æå– system å’Œ user å†…å®¹
        instructions = None
        input_content = []
        
        for msg in messages:
            role = msg.get('role', '')
            content = msg.get('content', '')
            
            if role == 'system':
                instructions = content
            elif role in ('user', 'assistant'):
                input_content.append({
                    "role": role,
                    "content": content
                })
        
        # å¦‚æœåªæœ‰ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œå¯ä»¥ç®€åŒ–ä¸ºå­—ç¬¦ä¸²
        if len(input_content) == 1 and input_content[0]['role'] == 'user':
            input_data = input_content[0]['content']
        else:
            input_data = input_content
        
        payload = {
            "model": self.model,
            "input": input_data,
            "temperature": temperature,
        }
        
        if instructions:
            payload["instructions"] = instructions
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        try:
            response = requests.post(url, json=payload, headers=headers, timeout=120)
            response.raise_for_status()
            
            data = response.json()
            
            # ä» Responses API æ ¼å¼ä¸­æå–æ–‡æœ¬
            # output æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œæ‰¾åˆ° type="message" çš„é¡¹
            output = data.get('output', [])
            for item in output:
                if item.get('type') == 'message':
                    content = item.get('content', [])
                    for c in content:
                        if c.get('type') == 'output_text':
                            return c.get('text', '')
            
            # å°è¯• output_text å¿«æ·æ–¹å¼
            if 'output_text' in data:
                return data['output_text']
            
            return str(data)
            
        except requests.exceptions.RequestException as e:
            raise Exception(f"Responses API è¯·æ±‚å¤±è´¥: {e}")
    
    def get_model_name(self) -> str:
        return self.model


class LLMCache:
    """LLMå“åº”ç¼“å­˜"""
    
    def __init__(self, cache_dir: str = None):
        """
        åˆå§‹åŒ–ç¼“å­˜
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
        """
        if cache_dir is None:
            cache_dir = os.path.join(os.path.dirname(__file__), '.llm_cache')
        
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def _get_cache_key(self, messages: list, model: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = json.dumps(messages, sort_keys=True) + model
        return hashlib.sha256(content.encode()).hexdigest()
    
    def get(self, messages: list, model: str) -> Optional[str]:
        """è·å–ç¼“å­˜"""
        key = self._get_cache_key(messages, model)
        cache_file = os.path.join(self.cache_dir, f"{key}.json")
        
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('response')
        return None
    
    def set(self, messages: list, model: str, response: str):
        """è®¾ç½®ç¼“å­˜"""
        key = self._get_cache_key(messages, model)
        cache_file = os.path.join(self.cache_dir, f"{key}.json")
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump({
                'messages': messages,
                'model': model,
                'response': response
            }, f, ensure_ascii=False, indent=2)
    
    def clear(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        import shutil
        if os.path.exists(self.cache_dir):
            shutil.rmtree(self.cache_dir)
            os.makedirs(self.cache_dir)


class CachedLLMClient:
    """å¸¦ç¼“å­˜çš„LLMå®¢æˆ·ç«¯åŒ…è£…å™¨"""
    
    def __init__(self, client: BaseLLMClient, enable_cache: bool = True):
        """
        åˆå§‹åŒ–
        
        Args:
            client: LLMå®¢æˆ·ç«¯
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        self.client = client
        self.enable_cache = enable_cache
        self.cache = LLMCache() if enable_cache else None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_calls': 0,
            'cache_hits': 0,
            'api_calls': 0
        }
    
    def chat(self, messages: list, temperature: float = 0.1,
             response_format: str = "json", use_cache: bool = True) -> str:
        """
        å‘é€èŠå¤©è¯·æ±‚ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            response_format: å“åº”æ ¼å¼
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆå¯¹äºæ­¤æ¬¡è¯·æ±‚ï¼‰
        """
        self.stats['total_calls'] += 1
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if self.enable_cache and use_cache and temperature == 0.1:
            cached = self.cache.get(messages, self.client.get_model_name())
            if cached:
                self.stats['cache_hits'] += 1
                return cached
        
        # è°ƒç”¨API
        self.stats['api_calls'] += 1
        response = self.client.chat(messages, temperature, response_format)
        
        # å­˜å…¥ç¼“å­˜
        if self.enable_cache and use_cache and temperature == 0.1:
            self.cache.set(messages, self.client.get_model_name(), response)
        
        return response
    
    def get_model_name(self) -> str:
        return self.client.get_model_name()
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š LLMè°ƒç”¨ç»Ÿè®¡:")
        print(f"   æ€»è°ƒç”¨: {self.stats['total_calls']}")
        print(f"   ç¼“å­˜å‘½ä¸­: {self.stats['cache_hits']}")
        print(f"   å®é™…APIè°ƒç”¨: {self.stats['api_calls']}")
        if self.stats['total_calls'] > 0:
            hit_rate = self.stats['cache_hits'] / self.stats['total_calls'] * 100
            print(f"   ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}%")


def create_llm_client(model_type: str = "large", 
                      api_key: str = None,
                      base_url: str = None,
                      enable_cache: bool = True) -> CachedLLMClient:
    """
    åˆ›å»ºLLMå®¢æˆ·ç«¯çš„å·¥å‚å‡½æ•°
    
    Args:
        model_type: "large" (GPT-4çº§åˆ«) æˆ– "small" (GPT-3.5çº§åˆ«)
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
        enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
    
    Returns:
        CachedLLMClient å®ä¾‹
    """
    if model_type == "large":
        model = "gpt-4"  # æˆ– "gpt-4-turbo-preview"
    else:
        model = "gpt-3.5-turbo"
    
    client = OpenAIClient(api_key=api_key, model=model, base_url=base_url)
    return CachedLLMClient(client, enable_cache=enable_cache)
