#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LLM-Enhanced V-SZZ è¿è¡Œç¤ºä¾‹
æ”¯æŒæ··åˆæ¨¡å¼ï¼ˆAST/srcml + LLMï¼‰å’Œçº¯LLMæ¨¡å¼
"""

import os
import sys
import argparse

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from llm_vszz import analyze_fix_commit, create_llm_enhanced_vszz


def main():
    parser = argparse.ArgumentParser(
        description='LLM-Enhanced V-SZZ: ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å¢å¼ºçš„æ¼æ´å¼•å…¥æäº¤è¿½è¸ª',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨æ··åˆæ¨¡å¼åˆ†æï¼ˆé»˜è®¤ï¼šAST/srcml + LLMï¼‰
  python run_llm_vszz.py /path/to/repo abc123def456
  
  # ä½¿ç”¨çº¯LLMæ¨¡å¼ï¼ˆä¸ä½¿ç”¨AST/srcmlå·¥å…·ï¼‰
  python run_llm_vszz.py /path/to/repo abc123 --pure-llm
  
  # æŒ‡å®šAPIå¯†é’¥
  python run_llm_vszz.py /path/to/repo abc123 --api-key sk-xxx
  
  # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹
  python run_llm_vszz.py /path/to/repo abc123 --large-model gpt-5.2 --small-model gpt-4.1-mini
  
  # ç¦ç”¨ç¼“å­˜
  python run_llm_vszz.py /path/to/repo abc123 --no-cache

æ··åˆæ¨¡å¼å·¥ä½œæµç¨‹:
  1. Javaæ–‡ä»¶ â†’ ASTå·¥å…·(ASTMapEval.jar)åˆ†æ â†’ LLMéªŒè¯/å¢å¼º
  2. C/C++æ–‡ä»¶ â†’ srcmlå·¥å…·åˆ†æ â†’ LLMéªŒè¯/å¢å¼º
  3. å…¶ä»–æ–‡ä»¶ â†’ ç›´æ¥ä½¿ç”¨LLMåˆ†æ
  4. å°LLMéªŒè¯æœ€ç»ˆç»“æœ

ç¯å¢ƒå˜é‡:
  OPENAI_API_KEY: APIå¯†é’¥ (äº‘é›¾APIæˆ–OpenAI)
  OPENAI_BASE_URL: APIåŸºç¡€URL (é»˜è®¤ä½¿ç”¨äº‘é›¾API: https://yunwu.ai/v1)
        """
    )
    
    parser.add_argument('repo_path', help='Gitä»“åº“è·¯å¾„')
    parser.add_argument('fix_commit', help='ä¿®å¤æäº¤çš„å“ˆå¸Œå€¼')
    
    parser.add_argument('--api-key', help='OpenAI APIå¯†é’¥ (æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY)')
    parser.add_argument('--base-url', default='https://yunwu.ai/v1',
                       help='APIåŸºç¡€URL (é»˜è®¤: https://yunwu.ai/v1)')
    
    parser.add_argument('--large-model', default='gpt-5.1-codex',
                       help='å¤§æ¨¡å‹åç§°ï¼Œç”¨äºè¿½è¸ªå†³ç­– (é»˜è®¤: gpt-5.1-codex)')
    parser.add_argument('--small-model', default='gpt-5-mini',
                       help='å°æ¨¡å‹åç§°ï¼Œç”¨äºç»“æœéªŒè¯ (é»˜è®¤: gpt-5-mini)')
    
    # æ··åˆæ¨¡å¼ç›¸å…³å‚æ•°
    parser.add_argument('--pure-llm', action='store_true',
                       help='ä½¿ç”¨çº¯LLMæ¨¡å¼ï¼ˆä¸ä½¿ç”¨AST/srcmlå·¥å…·ï¼‰')
    parser.add_argument('--ast-path', 
                       help='ASTå·¥å…·è·¯å¾„ (ASTMapEval.jaræ‰€åœ¨ç›®å½•)')
    
    parser.add_argument('--no-cache', action='store_true',
                       help='ç¦ç”¨LLMå“åº”ç¼“å­˜')
    parser.add_argument('--max-depth', type=int, default=30,
                       help='æœ€å¤§è¿½è¸ªæ·±åº¦ (é»˜è®¤: 30)')
    parser.add_argument('--max-iterations', type=int, default=3,
                       help='éªŒè¯å¤±è´¥åæœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 3)')
    
    parser.add_argument('--output', '-o', help='è¾“å‡ºç»“æœåˆ°JSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # éªŒè¯ä»“åº“è·¯å¾„
    if not os.path.exists(args.repo_path):
        print(f"âŒ ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {args.repo_path}")
        sys.exit(1)
    
    if not os.path.exists(os.path.join(args.repo_path, '.git')):
        print(f"âŒ ä¸æ˜¯æœ‰æ•ˆçš„Gitä»“åº“: {args.repo_path}")
        sys.exit(1)
    
    # è·å–APIå¯†é’¥
    api_key = args.api_key or os.environ.get('OPENAI_API_KEY')
    if not api_key:
        print("âŒ æœªé…ç½®APIå¯†é’¥")
        print("   è¯·ä½¿ç”¨ --api-key å‚æ•°æˆ–è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # è·å–base_urlï¼ˆé»˜è®¤ä½¿ç”¨äº‘é›¾APIï¼‰
    base_url = args.base_url
    if base_url is None:
        base_url = os.environ.get('OPENAI_BASE_URL', 'https://yunwu.ai/v1')
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨æ··åˆæ¨¡å¼
    use_hybrid = not args.pure_llm
    
    # ç¡®å®šASTå·¥å…·è·¯å¾„
    ast_path = args.ast_path
    if ast_path is None and use_hybrid:
        # é»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„ ASTMapEval_jar
        default_ast_path = os.path.join(current_dir, 'ASTMapEval_jar')
        if os.path.exists(default_ast_path):
            ast_path = default_ast_path
    
    try:
        # è¿è¡Œåˆ†æ
        results = analyze_fix_commit(
            repo_path=args.repo_path,
            fix_commit_hash=args.fix_commit,
            api_key=api_key,
            large_model=args.large_model,
            small_model=args.small_model,
            use_hybrid=use_hybrid,
            ast_map_path=ast_path
        )
        
        # ä¿å­˜ç»“æœ
        if args.output:
            import json
            output_data = []
            for r in results:
                output_data.append({
                    'fix_commit': r.fix_commit,
                    'bic_commit': r.bic_commit,
                    'verified': r.verified,
                    'iterations': r.iterations,
                    'tracking_chain': [
                        {
                            'commit_hash': s.commit_hash,
                            'commit_date': s.commit_date,
                            'commit_message': s.commit_message,
                            'file_path': s.file_path,
                            'line_num': s.line_num,
                            'change_type': s.change_type,
                            'reasoning': s.reasoning,
                            'confidence': s.confidence
                        }
                        for s in r.tracking_chain
                    ]
                })
            
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        
        print("\nâœ… åˆ†æå®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
